{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IheRzy0LT82v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import VitsModel, AutoTokenizer, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import librosa\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(audio_path):\n",
        "  from google.colab import userdata\n",
        "\n",
        "  hf_token = userdata.get('HF_TOKEN')\n",
        "  processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\", token=hf_token )\n",
        "  model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", ignore_mismatched_sizes = True, token=hf_token)\n",
        "\n",
        "  audio , sr = librosa.load(audio_path, sr=16000)\n",
        "  inputs = processor(audio, sampling_rate = sr, return_tensors = \"pt\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = model(inputs.input_values).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim = -1)\n",
        "  transcription = processor.batch_decode(predicted_ids)\n",
        "\n",
        "  return transcription[0]"
      ],
      "metadata": {
        "id": "hZh6c1HaUKIx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = transcribe_audio(\"audio2.wav\")\n",
        "print(f\"Transcription: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5U7GYBMUKwW",
        "outputId": "47f17c7b-8466-489c-b2d5-68cdc6ab5511"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription: ALLO THERE THIS IS JACK AND ARAN TRYING THIS MUDDLE FOR THE FIRST TIME\n"
          ]
        }
      ]
    }
  ]
}